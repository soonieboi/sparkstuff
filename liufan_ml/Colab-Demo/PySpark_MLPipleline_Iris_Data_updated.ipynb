{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCIGWnMMtEuv",
        "outputId": "8a7b919b-5abc-4e7c-c81d-cdce1877eece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "java-1.21.0-openjdk-amd64\n",
            "java-21-openjdk-amd64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. Install OpenJDK 21 (if not already done in a previous cell)\n",
        "!apt-get update -qq\n",
        "!apt-get install -qq openjdk-21-jdk-headless\n",
        "\n",
        "# 2. Verify where it landed (if needed)\n",
        "!ls /usr/lib/jvm | grep 21\n",
        "\n",
        "# 3. Point to JDK 21\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "# 4. Install PySpark via pip (make sure this happens AFTER setting JAVA_HOME)\n",
        "!pip install pyspark --quiet\n",
        "\n",
        "# 5. Import and start Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "      .master(\"local[*]\")\n",
        "      .appName(\"ML-Pipleline_IrisData\")\n",
        "      .getOrCreate()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGOaOkpitX0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3cMSV9GteWJ",
        "outputId": "6d4dbe21-a652-4b44-ca35-f27b6afb8963"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType\n",
        "from sklearn.datasets import load_iris\n",
        "from pyspark.ml.feature import StringIndexer\n"
      ],
      "metadata": {
        "id": "tk0jTUFitx-k"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset using scikit-learn\n",
        "iris = load_iris()\n",
        "iris_data = iris.data\n",
        "iris_target = iris.target"
      ],
      "metadata": {
        "id": "L-36_ajEt4z8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"sepal_length\", DoubleType(), True),\n",
        "    StructField(\"sepal_width\", DoubleType(), True),\n",
        "    StructField(\"petal_length\", DoubleType(), True),\n",
        "    StructField(\"petal_width\", DoubleType(), True),\n",
        "    StructField(\"label\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a DataFrame with the specified schema\n",
        "data = spark.createDataFrame(\n",
        "    [(float(x[0]), float(x[1]), float(x[2]), float(x[3]), int(y)) for x, y in zip(iris_data, iris_target)],\n",
        "    schema=schema\n",
        ")"
      ],
      "metadata": {
        "id": "_QT1zppEt43a"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qa4vSNTuAns",
        "outputId": "39217f15-ad2b-4267-a494-ddd90c087dc9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|label|\n",
            "+------------+-----------+------------+-----------+-----+\n",
            "|         5.1|        3.5|         1.4|        0.2|    0|\n",
            "|         4.9|        3.0|         1.4|        0.2|    0|\n",
            "|         4.7|        3.2|         1.3|        0.2|    0|\n",
            "|         4.6|        3.1|         1.5|        0.2|    0|\n",
            "|         5.0|        3.6|         1.4|        0.2|    0|\n",
            "|         5.4|        3.9|         1.7|        0.4|    0|\n",
            "|         4.6|        3.4|         1.4|        0.3|    0|\n",
            "|         5.0|        3.4|         1.5|        0.2|    0|\n",
            "|         4.4|        2.9|         1.4|        0.2|    0|\n",
            "|         4.9|        3.1|         1.5|        0.1|    0|\n",
            "|         5.4|        3.7|         1.5|        0.2|    0|\n",
            "|         4.8|        3.4|         1.6|        0.2|    0|\n",
            "|         4.8|        3.0|         1.4|        0.1|    0|\n",
            "|         4.3|        3.0|         1.1|        0.1|    0|\n",
            "|         5.8|        4.0|         1.2|        0.2|    0|\n",
            "|         5.7|        4.4|         1.5|        0.4|    0|\n",
            "|         5.4|        3.9|         1.3|        0.4|    0|\n",
            "|         5.1|        3.5|         1.4|        0.3|    0|\n",
            "|         5.7|        3.8|         1.7|        0.3|    0|\n",
            "|         5.1|        3.8|         1.5|        0.3|    0|\n",
            "+------------+-----------+------------+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=1234)"
      ],
      "metadata": {
        "id": "VOxpIgb1uAqb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature columns\n",
        "feature_columns = data.columns\n",
        "feature_columns.remove(\"label\")\n",
        "\n",
        "# Create a vector assembler to assemble feature columns into a single feature vector\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Create a StringIndexer to convert labels to indices\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")"
      ],
      "metadata": {
        "id": "fpjt5RE5t46k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")"
      ],
      "metadata": {
        "id": "j9oaIoEuuLIk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline with the stages: vector assembler, label indexer, and random forest\n",
        "pipeline = Pipeline(stages=[assembler, indexer, rf])"
      ],
      "metadata": {
        "id": "v3VAL9BtuLLQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.maxDepth, [5, 10, 15])\n",
        "             .addGrid(rf.numTrees, [20, 50, 100])\n",
        "             .build())"
      ],
      "metadata": {
        "id": "h5jIQ8o0uLOc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a multi-class classification evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Create a cross-validator with the pipeline, parameter grid, and evaluator\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "\n",
        "# Fit the cross-validator to the training data\n",
        "cvModel = crossval.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = %g\" % accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57S2XYiFuRtl",
        "outputId": "f99835cb-2ba7-4324-8dd1-a601d9ac104d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.945946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "HyjoUcw0uRwl"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}