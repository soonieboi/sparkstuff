{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soonieboi/sparkstuff/blob/main/04_REWORK_Ingestion_and_Data_Formats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "PySpark provides interface used to load DataFrame from external storage systems. We will learn how to read different data format files into DataFrame and write DataFrame back to different data format files using PySpark examples. Lastly, we will learn how to transfer data between JVM and Python processes using Apache Arrow efficiently."
      ],
      "metadata": {
        "id": "rlDOUpggmTDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Install OpenJDK 21 (if not already done in a previous cell)\n",
        "!apt-get update -qq\n",
        "!apt-get install -qq openjdk-21-jdk-headless\n",
        "\n",
        "# 2. Verify where it landed (if needed)\n",
        "!ls /usr/lib/jvm | grep 21\n",
        "\n",
        "# 3. Point to JDK 21\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "# 4. Install PySpark via pip (make sure this happens AFTER setting JAVA_HOME)\n",
        "!pip install pyspark --quiet\n",
        "\n",
        "# 5. Import and start Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "      .master(\"local[*]\")\n",
        "      .appName(\"Spark on Java21\")\n",
        "      .getOrCreate()\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYlYPSOJmGmY",
        "outputId": "34d464e5-6564-4d8d-b9cc-a1a7dbbf68e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package openjdk-21-jre-headless:amd64.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-21-jre-headless_21.0.8+9~us1-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-21-jre-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-21-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-21-jdk-headless_21.0.8+9~us1-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-21-jdk-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "Setting up openjdk-21-jre-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-21-jdk-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jwebserver to provide /usr/bin/jwebserver (jwebserver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "java-1.21.0-openjdk-amd64\n",
            "java-21-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RKyb29amQVT",
        "outputId": "9e96f716-8b76-40d9-aa5e-9fc2f212b667"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read CSV file\n",
        "PySpark provides DataFrameReader to load a DataFrame from external storage systems (e.g. file systems, key-value stores, etc). Use SparkSession.read to access this. You can use format(source) to specify the input data source format.\n",
        "Using csv(\"path\") or format(\"csv\").load(\"path\") of DataFrameReader, you can read a CSV file into a PySpark DataFrame, These methods take a file path to read from as an argument. When you use format(\"csv\") method, you can also specify the data sources by their fully qualified name, but for built-in sources, you can simply use their short names (csv,json, parquet, jdbc, text e.t.c). In this example, it shows how to read a single CSV file “people.csv” into DataFrame as well as how to use your own defined schema when read file into DataFrame."
      ],
      "metadata": {
        "id": "Wg3kffrtmjfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV file people.csv\n",
        "df = spark.read.format('csv') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"/content/drive/MyDrive/data/DataFormat/people.csv\")\n",
        "\n",
        "# Show result\n",
        "df.show()\n",
        "# Print schema\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsjSoFjVmg7s",
        "outputId": "06d396e4-232d-4da7-fa42-133540255f61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---------+\n",
            "| name|age|      job|\n",
            "+-----+---+---------+\n",
            "|Jorge| 30|Developer|\n",
            "|  Bob| 32|Developer|\n",
            "+-----+---+---------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write csv file\n",
        "df = spark.read.format('csv') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"/content/drive/MyDrive/data/DataFormat/people.csv\")\n"
      ],
      "metadata": {
        "id": "tNjqQFj0mrDs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('json') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"/content/drive/MyDrive/data/DataFormat/people.json\")\n",
        "df.show()\n",
        "df.printSchema\n",
        "\n",
        "peopleDF = spark.read.json(\"/content/drive/MyDrive/data/DataFormat/people.json\")\n",
        "peopleDF.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYaE7Nt6m0Qp",
        "outputId": "8f53075a-015e-4bc9-88a6-90d99e75d9f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+-------+\n",
            "| age|      job|   name|\n",
            "+----+---------+-------+\n",
            "|NULL|     NULL|Michael|\n",
            "|  30|developer|   Andy|\n",
            "|  19|     NULL| Justin|\n",
            "+----+---------+-------+\n",
            "\n",
            "+----+---------+-------+\n",
            "| age|      job|   name|\n",
            "+----+---------+-------+\n",
            "|NULL|     NULL|Michael|\n",
            "|  30|developer|   Andy|\n",
            "|  19|     NULL| Justin|\n",
            "+----+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
        "peopleDF.write.format(\"parquet\").mode(\"overwrite\").save(\"people.parquet\")\n"
      ],
      "metadata": {
        "id": "T-j-Kn_nm2g6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the Parquet file created above.\n",
        "# Parquet files are self-describing so the schema is preserved.\n",
        "# The result of loading a parquet file is also a DataFrame.\n",
        "parquetFile = spark.read.parquet(\"people.parquet\")\n",
        "\n",
        "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
        "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
        "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
        "teenagers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJUBatvsm5W6",
        "outputId": "6267f3c4-5089-4dbf-fc3d-ecac9c472c1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "|Justin|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.csv as pv\n",
        "import pyarrow.parquet as pq\n",
        "# read hdb resale price\n",
        "hdb_table = pv.read_csv(\"/content/drive/MyDrive/data/DataFormat/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\")\n",
        "# convert the CSV file to a Parquet file\n",
        "pq.write_table(hdb_table,'resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
        "hdb_parquet = pq.ParquetFile('resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
        "# inspect the parquet metadata\n",
        "print(hdb_parquet.metadata)\n",
        "# inspect the parquet row group metadata\n",
        "print(hdb_parquet.metadata.row_group(0))\n",
        "# inspect the column chunk metadata\n",
        "print(hdb_parquet.metadata.row_group(0).column(9).statistics)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXeusEC7m7Q5",
        "outputId": "44f57134-c1f0-4a6d-b6ff-0d09f60bf814"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyarrow._parquet.FileMetaData object at 0x7ee0b1de11c0>\n",
            "  created_by: parquet-cpp-arrow version 18.1.0\n",
            "  num_columns: 10\n",
            "  num_rows: 52203\n",
            "  num_row_groups: 1\n",
            "  format_version: 2.6\n",
            "  serialized_size: 2061\n",
            "<pyarrow._parquet.RowGroupMetaData object at 0x7ee0940dd8a0>\n",
            "  num_columns: 10\n",
            "  num_rows: 52203\n",
            "  total_byte_size: 431095\n",
            "  sorting_columns: ()\n",
            "<pyarrow._parquet.Statistics object at 0x7ee0940dd8f0>\n",
            "  has_min_max: True\n",
            "  min: 195000.0\n",
            "  max: 1088888.0\n",
            "  null_count: 0\n",
            "  distinct_count: None\n",
            "  num_values: 52203\n",
            "  physical_type: DOUBLE\n",
            "  logical_type: None\n",
            "  converted_type (legacy): NONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mVGUtD7_m0GK"
      }
    }
  ]
}