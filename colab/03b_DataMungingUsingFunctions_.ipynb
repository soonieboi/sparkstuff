{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/jjw8okQaDB6yLHG4Aapv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2025/blob/main/colab/03b_DataMungingUsingFunctions_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Munging Examples\n",
        "Data munging (or data wrangling) in PySpark for taxi booking services typically involves cleaning, transforming, and enriching raw data to make it suitable for analysis.\n",
        "Here are some common data munging tasks with examples:"
      ],
      "metadata": {
        "id": "Qi8hBz_n9l3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Spark"
      ],
      "metadata": {
        "id": "J7FtUeDXXrY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install pyspark using pip\n",
        "!pip install --ignore-install -q pyspark\n",
        "# install findspark using pip\n",
        "!pip install --ignore-install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-KDz-CkXtey",
        "outputId": "041f9646-fccf-4114-aa43-b41571f6241c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SparkSession\n",
        "Start Spark Session"
      ],
      "metadata": {
        "id": "6qYGMkfBXt9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUYiRid59VYB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"My First PySpark App\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drive\n",
        "Connect to Google Drive"
      ],
      "metadata": {
        "id": "9PWsaqGeYJ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "from google.colab import drive\n",
        "# to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEzzDmlBcrNH",
        "outputId": "65950fb4-bb6d-453f-8654-d4b2a2be64b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taxi Data Munging\n",
        "Data munging (or data wrangling) in PySpark for taxi booking services typically involves cleaning, transforming, and enriching raw data to make it suitable for analysis. Here are some common data munging tasks with examples:\n",
        "\n",
        "### Handling Missing Values\n",
        "Taxi booking datasets often contain missing values in fields like dropoff_location, fare_amount, etc."
      ],
      "metadata": {
        "id": "QBvEM2L7cydp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TaxiDataMunging\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (1, \"2025-02-18 10:00:00\", \"2025-02-18 10:30:00\", None, 15.5),\n",
        "    (2, \"2025-02-18 11:00:00\", None, \"Downtown\", 20.0),\n",
        "    (3, None, \"2025-02-18 12:00:00\", \"Airport\", None),\n",
        "]\n",
        "columns = [\"trip_id\", \"pickup_time\", \"dropoff_time\", \"dropoff_location\", \"fare_amount\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n"
      ],
      "metadata": {
        "id": "Wk0Kz0hQc1Zw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill missing values\n",
        "In PySpark you can handle missing (null or NaN) values in a couple of ways:\n",
        "\n",
        "1. Using the DataFrame’s fillna() Method\n",
        "This method lets you replace missing values with a specified constant. You can either fill every column with the same value or pass a dictionary to specify different fill values per column.\n",
        "2. Using the Imputer from pyspark.ml.feature\n",
        "For numerical columns, you might prefer to impute missing values with a statistic such as the mean or median.\n",
        "\n",
        "Both methods are commonly used depending on the nature of your data and the imputation strategy you need.\n",
        "\n",
        "fillna() / na.fill(): Use these for simple replacements where a constant or specific value per column is appropriate.\n",
        "\n",
        "Imputer: Best for numerical columns when you want a data-driven replacement (e.g., mean or median)."
      ],
      "metadata": {
        "id": "arTLiz-xXw5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.fillna({\"dropoff_location\": \"Unknown\", \"fare_amount\": 0.0})\n",
        "\n",
        "df_cleaned.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCB1hgRVzjQi",
        "outputId": "6cfecfaf-f9da-43db-f792-b5b31457c0d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|         Unknown|       15.5|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|        0.0|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Datetime Formats\n",
        "Timestamps are usually in string format. We convert them to PySpark Timestamp for easy calculations. Below example converts time fields from string to timestamp for further processing.\n"
      ],
      "metadata": {
        "id": "jzo9PyDuzm0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "df = df.withColumn(\"pickup_time\", to_timestamp(col(\"pickup_time\")))\n",
        "df = df.withColumn(\"dropoff_time\", to_timestamp(col(\"dropoff_time\")))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5nPWoEezreq",
        "outputId": "abafc98d-4562-47e6-9d4e-b9b451618461"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|       NULL|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering Invalid Data\n",
        "Remove bookings with negative fares or missing crucial data. Below example keeps only valid trips where fare is positive and pickup time is not missing.\n"
      ],
      "metadata": {
        "id": "vnUgR4JJztDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = df.filter((col(\"fare_amount\") > 0) & col(\"pickup_time\").isNotNull())\n",
        "df_valid.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UzFu2cdz6Qq",
        "outputId": "4f5c8eee-051d-4d17-87dd-3506dc8d907e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "This example focuses on Trip Duration. The computation is in minutes."
      ],
      "metadata": {
        "id": "gtqpMoOOz-Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import unix_timestamp, col\n",
        "\n",
        "df = df.withColumn(\"trip_duration_mins\", \\\n",
        "                   (unix_timestamp(col(\"dropoff_time\")) -  \\\n",
        "                    unix_timestamp(col(\"pickup_time\"))) / 60)\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drz3D7PA0LuK",
        "outputId": "8ac19ed0-ab66-434e-b154-d5322317c879"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|trip_duration_mins|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|              30.0|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|              NULL|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|       NULL|              NULL|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geospatial Filtering (Pickups in Specific Area)\n",
        "Filter trips that started from a given region (e.g., Manhattan). Selects trips that ended in Changi.\n"
      ],
      "metadata": {
        "id": "-Se92B2r0Zyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df.filter(col(\"dropoff_location\") == \"Changi\")\n",
        "df_filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiVznmUq0gc5",
        "outputId": "cc368687-7268-4832-fe81-5adc111190c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+------------+----------------+-----------+------------------+\n",
            "|trip_id|pickup_time|dropoff_time|dropoff_location|fare_amount|trip_duration_mins|\n",
            "+-------+-----------+------------+----------------+-----------+------------------+\n",
            "+-------+-----------+------------+----------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregation\n",
        "Summarize daily revenue. Calculates total fare collected per day."
      ],
      "metadata": {
        "id": "wHmOqwX70wXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_format, sum\n",
        "\n",
        "df_daily_fare = df.withColumn(\"date\", date_format(col(\"pickup_time\"), \"yyyy-MM-dd\")) \\\n",
        "                  .groupBy(\"date\").agg(sum(\"fare_amount\").alias(\"total_fare\"))\n",
        "\n",
        "df_daily_fare.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyR-o7E_1IqA",
        "outputId": "abe059cb-e96b-4bfc-ab3d-3d1c287b3bac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|      date|total_fare|\n",
            "+----------+----------+\n",
            "|2025-02-18|      35.5|\n",
            "|      NULL|      NULL|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Deduplication\n",
        "\n"
      ],
      "metadata": {
        "id": "E_ESI-S-1Oko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped = df.dropDuplicates([\"trip_id\"])\n",
        "df_deduped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj2md2AI1WMw",
        "outputId": "dd28634a-fe07-469b-c58c-f371cc4aecf0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|trip_duration_mins|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|              30.0|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|              NULL|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|       NULL|              NULL|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bucketing Fare Amounts\n",
        "Categorize trips based on fare price. Groups fares into Low, Medium, and High categories.\n"
      ],
      "metadata": {
        "id": "JIIZatI71c9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "df = df.withColumn(\"fare_category\", when(col(\"fare_amount\") < 10, \"Low Fare\") \\\n",
        "       .when((col(\"fare_amount\") >= 10) & (col(\"fare_amount\") < 30),  \\\n",
        "             \"Medium Fare\").otherwise(\"High Fare\"))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQyHEZ4N1hLQ",
        "outputId": "60ed61b0-048b-4bf2-9f8b-c1ba06c65916"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+------------------+-------------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|trip_duration_mins|fare_category|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+-------------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|              30.0|  Medium Fare|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|              NULL|  Medium Fare|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|       NULL|              NULL|    High Fare|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join Data\n",
        "Merge booking details with customer data."
      ],
      "metadata": {
        "id": "HIUQGBH614eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
        "customer_df = spark.createDataFrame(customer_data, [\"customer_id\", \"customer_name\"])\n",
        "\n",
        "trip_data = [(1, 1, 20.5), (2, 2, 15.0), (3, 1, 30.0)]\n",
        "trip_df = spark.createDataFrame(trip_data, [\"trip_id\", \"customer_id\", \"fare_amount\"])\n",
        "\n",
        "df_joined = trip_df.join(customer_df, \"customer_id\", \"inner\")\n",
        "df_joined.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2w2jXyb16XA",
        "outputId": "e1617328-d025-4d4b-cfa2-ecf03462d547"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-----------+-------------+\n",
            "|customer_id|trip_id|fare_amount|customer_name|\n",
            "+-----------+-------+-----------+-------------+\n",
            "|          1|      1|       20.5|        Alice|\n",
            "|          1|      3|       30.0|        Alice|\n",
            "|          2|      2|       15.0|          Bob|\n",
            "+-----------+-------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Data\n",
        "Save the cleaned dataset as Parquet for efficient querying. Stores processed data in Parquet format."
      ],
      "metadata": {
        "id": "cSfUe7pS16zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.write.mode(\"overwrite\").parquet(\"output/taxi_cleaned.parquet\")"
      ],
      "metadata": {
        "id": "64HhzTKu2AG_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Thoughts\n",
        "These PySpark data munging techniques help clean, filter, and transform raw taxi booking data, making it ready for analytics and reporting.\n",
        "Would you like an end-to-end PySpark pipeline for this? 🚀\n",
        "\n",
        "Yes, you can install Python and PyCharm on a Windows machine and run PySpark code within a Podman Kubernetes cluster.\n"
      ],
      "metadata": {
        "id": "9u3Viw671AaQ"
      }
    }
  ]
}